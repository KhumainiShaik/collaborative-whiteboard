# Kubernetes Monitoring & HPA Setup - Documentation

## Quick Navigation

This workspace contains comprehensive documentation for a production-grade Kubernetes monitoring and autoscaling setup. Choose the document that matches your need:

### ğŸ“‹ [FRESH_SETUP.md](./FRESH_SETUP.md)
**For:** Setting up from scratch or debugging deployment issues.

**Contains:**
- 6-phase deployment walkthrough with verification at each step
- Common issues and fixes (Grafana datasource, cAdvisor crashes, etc.)
- Manual verification commands
- Troubleshooting checklist

**Start here if:** You're deploying the cluster for the first time or need to understand what each manifest does.

---

### ğŸ” [OBSERVABILITY.md](./OBSERVABILITY.md)
**For:** Understanding monitoring architecture and querying metrics.

**Contains:**
- System architecture (Prometheus â†’ kube-state-metrics â†’ Grafana)
- Component reference (each exporter's purpose and metrics)
- PromQL patterns (CPU, memory, network, pod count queries)
- Troubleshooting: missing data, cardinality errors, metric discovery

**Start here if:** You need to debug "no data in dashboard," add new panels, or understand metric sources.

---

### ğŸ¯ [TASK4_DEMO.md](./TASK4_DEMO.md)
**For:** Running a live demo of HPA, Yjs sync, and monitoring.

**Contains:**
- Step-by-step demo walkthrough (~15 min)
- Load generation, HPA scaling observation, dashboard live view
- Yjs synchronization verification
- Talk track and Q&A topics
- Success criteria and copy-paste commands

**Start here if:** You're presenting the system or testing autoscaling behavior.

---

## Files Reference

### Kubernetes Manifests (`k8s-manifests/`)
Essential manifests for this setup (listed in deployment order):

| Manifest | Purpose | Status |
|----------|---------|--------|
| `00-namespace.yaml` | Create whiteboard namespace | âœ… Applied |
| `01-mongodb-secret.yaml` | MongoDB credentials | âœ… Applied |
| `02-kcc-pubsub.yaml` | GCP Pub/Sub integration | âœ… Applied |
| `03-kcc-gcs.yaml` | GCP GCS snapshot support | âœ… Applied |
| `04-k8s-serviceaccount.yaml` | Service account for workloads | âœ… Applied |
| `05-yjs-deployment.yaml` | Yjs server (WebSocket sync) | âœ… Applied |
| `06-excalidraw-deployment.yaml` | Excalidraw UI (frontend) | âœ… Applied |
| `07-network-policy.yaml` | Pod network isolation | âœ… Applied |
| `08-ingress-tls.yaml` | HTTPS ingress rules | âœ… Applied |
| `11-nginx-ingress.yaml` | NGINX ingress controller | âœ… Applied |
| `12-prometheus.yaml` | Prometheus scraper + config | âœ… Applied |
| `13-grafana.yaml` | Grafana UI + datasource config | âœ… Applied |
| `14-hpa.yaml` | HPA resources (autoscaling) | âœ… Applied |
| `15-metrics-server.yaml` | Metrics server (HPA data) | âœ… Applied |
| `15-node-exporter.yaml` | Node-exporter DaemonSet | âœ… Applied |
| `16-cadvisor.yaml` | cAdvisor DaemonSet | âœ… Applied |
| `16-grafana-ingress.yaml` | Grafana ingress rule | âœ… Applied |
| `ksm-rbac.yaml` | kube-state-metrics + RBAC | âœ… Applied |

**Note:** Redis StatefulSet and other prerequisites are included in the core manifests. See FRESH_SETUP.md for deployment order.

---

## Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Load Balancer                 â”‚
â”‚                   (GCP HTTP(S) LB)                        â”‚
â”‚                       â†“                                   â”‚
â”‚         http://<IP>/ (excalidraw app)                    â”‚
â”‚         http://<IP>/grafana/ (monitoring)                â”‚
â”‚         http://<IP>/api/ (backend API)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   NGINX Ingress Controller         â”‚
      â”‚   (routing, TLS termination)       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†™                    â†“                  â†–
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ excalidraw-uiâ”‚    â”‚  yjs-server     â”‚    â”‚   Grafana   â”‚
    â”‚ (3-8 replicas)    â”‚  (1 replica)    â”‚    â”‚  (frontend) â”‚
    â”‚  (HPA: CPU60%)    â”‚  (sessionAffinity)  â”‚  (Prometheus)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Service Mesh (optional)   â”‚
    â”‚  (network policies enforced)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Monitoring Stack                â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚  Prometheus (metrics store)  â”‚â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â”‚         â†™        â†™       â†™       â”‚
    â”‚    kube-state-  node-     cAdvisor
    â”‚    metrics      exporter   (containers)
    â”‚  (pod/HPA info) (node info)(limited)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key insight:** Metrics flow from exporters â†’ Prometheus â†’ Grafana. Each component is independently verifiable.

---

## Quick Troubleshooting

### "Grafana shows no data"
â†’ See [OBSERVABILITY.md: Troubleshooting](./OBSERVABILITY.md#troubleshooting) â†’ "Dashboard panels empty" section

### "Pods not scaling up"
â†’ See [FRESH_SETUP.md: Known Issues](./FRESH_SETUP.md#known-issues) â†’ "HPA metrics unhealthy" section

### "Yjs room state lost after scale"
â†’ See [FRESH_SETUP.md: Phase 2 Fix](./FRESH_SETUP.md#phase-2-yjs-sync-stability-fixed) â†’ Session affinity explanation

### "How do I add a new dashboard panel?"
â†’ See [OBSERVABILITY.md: PromQL Patterns](./OBSERVABILITY.md#promql-patterns) for query examples

---

## Deployment Sequence

If starting from scratch:

```bash
# 1. Prepare cluster (see FRESH_SETUP.md Phase 1)
kubectl apply -f k8s-manifests/00-namespace.yaml
kubectl apply -f k8s-manifests/01-mongodb-secret.yaml
# ... (other core resources)

# 2. Deploy workloads (see FRESH_SETUP.md Phase 2-3)
kubectl apply -f k8s-manifests/05-yjs-deployment.yaml
kubectl apply -f k8s-manifests/06-excalidraw-deployment.yaml

# 3. Setup monitoring (see FRESH_SETUP.md Phase 4-5)
kubectl apply -f k8s-manifests/12-prometheus.yaml
kubectl apply -f k8s-manifests/13-grafana.yaml
kubectl apply -f k8s-manifests/ksm-rbac.yaml
kubectl apply -f k8s-manifests/15-node-exporter.yaml
kubectl apply -f k8s-manifests/16-cadvisor.yaml

# 4. Configure HPA (see FRESH_SETUP.md Phase 6)
kubectl apply -f k8s-manifests/14-hpa.yaml

# 5. Verify (see FRESH_SETUP.md Verification)
kubectl get pods -n whiteboard -o wide
kubectl get hpa -n whiteboard
```

For detailed steps with verification, see **FRESH_SETUP.md**.

---

## Key Configuration Highlights

### HPA (Horizontal Pod Autoscaler)
- **excalidraw-ui:** minReplicas=3, maxReplicas=8, CPU 60%, Memory 75%
- **yjs-server:** minReplicas=1, maxReplicas=1 (no autoscale; singleton design)

**Why?** Yjs maintains in-memory room state; scaling loses state unless persisted to Redis. For this demo, we keep it singleton.

### Session Affinity (yjs-service)
- `sessionAffinity: ClientIP` ensures client requests route to same pod
- Prevents transient connections if pod flaps

### Grafana Config
- Served at `/grafana/` subdirectory (not root)
- Configured via `GF_SERVER_ROOT_URL` + `GF_SERVER_SERVE_FROM_SUB_PATH`
- Datasource: Prometheus (in-cluster at `http://prometheus-service.monitoring:9090`)

### Prometheus Scraping
- 15-second interval
- Service discovery: Kubernetes SD
- Targets: all pods with `prometheus.io/scrape: "true"` annotation

---

## Verification Checklist

Before running the demo:

- [ ] All pods running: `kubectl get pods -n whiteboard`
- [ ] HPA active: `kubectl get hpa -n whiteboard`
- [ ] Ingress has external IP: `kubectl get ingress -n whiteboard`
- [ ] Grafana accessible: `curl -I http://<IP>/grafana/`
- [ ] Prometheus targets up: Check Prometheus UI or Grafana datasource
- [ ] Dashboard panels have data: Open Grafana â†’ Dashboard â†’ Excalidraw System Monitoring

---

## Next Steps

1. **Deploy the cluster:** Follow FRESH_SETUP.md
2. **Understand metrics:** Read OBSERVABILITY.md
3. **Run the demo:** Follow TASK4_DEMO.md
4. **Extend monitoring:** Add custom panels or exporters as needed

---

## Support & Additional Context

- **Cluster:** k3s on GCP (3 nodes: 1 control, 2 workers)
- **Ingress:** NGINX controller behind HTTP Load Balancer
- **Namespace:** `whiteboard` (all workloads), `monitoring` (Prometheus/exporters), `ingress-nginx` (ingress)
- **Git:** Original Excalidraw repo + custom YJS integration + K8s manifests

For detailed architecture, component interactions, and troubleshooting, see the individual docs.
